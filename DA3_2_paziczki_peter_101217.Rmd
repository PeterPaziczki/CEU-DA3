---
title: "DA3_3_paziczki_peter_101217"
author: "Peter Paziczki"
date: "2017 December 10"
output:
  html_document: default
  pdf_document: default
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, echo=FALSE, warning=FALSE}
# Clearing memory
rm(list=ls())

# Setting global options
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r initial_settings, results='hide'}
# Loading necessary libraries
library(ggplot2)
library(data.table)
library(descr) # for the freq function
library(fBasics) # for basicStats function
library(lmtest) # for coeftest
library(sandwich) # sandwich is needed for coeftest function
library(stargazer) # needed to provide a nicer output for interpreting coefficients
library(pander) # pander function makes tables have a nicer output
library(mfx) # it is needed for computing marginal differences in probit and logit functions

# Setting presentation parameters
theme_update(plot.title = element_text(hjust = 0.5, size = 12))

# Checking and setting working directory
getwd()
setwd('/Users/User/Documents/R_projects/CEU-DA3')

options(digits=2)
```

# DA3 Assignment #3

### <span style="color:blue"> 1. In this exercise you have to use SHARE data and analyze the pattern of mortality rate with respect to physical activities, in an analysis similar to lecture 5

* Use the SHARE data file mortality_oldage_eu.csv 

* Filter data: Keep respondents between 50 and 80 years of age. The variables you will need are whether the person deceased within 6 years of the interview (?deceased?), gender (?female?), age (?age?), years of education (?eduyears_mod?), income group within country (?income10g), and the explanatory variables of your focus, physical activities (variable ?sports?: 1: more than once a week, 2: once a week, 3: one to three times a month, 4: hardly ever, or never).
</span>

```{r Q1_loading_data, results='hide'}
# Reading input files
share_raw <- read.csv("mortality_oldage_eu.csv", na.strings = ".")
share <- subset(share_raw, age>=50 & age<=80)
share <- subset(share, deceased!="." & female!="." & age!="." & eduyears_mod!="." & income10g!="." & sports!=".")

# Making it a data.table object
share <- data.table(share)

# Dropping the unnecessary columns, keeping only deceased, female, age, eduyears, income10g and sports.
# In exercise 4 we are asked to investigate other variables, so I keep other variables in the data set.
share <- share[,c('deceased', 'female', 'age', 'eduyears_mod', 'income10g', 'sports')]

# share <- share[,c('deceased', 'female', 'age', 'eduyears_mod', 'income10g', 'sports', 'drinks_never', 'drinks_everyday')]
```

SHARE (Survey of Health, Ageing and Retirement in Europe) data has been loaded from the csv file we have been provided (mortality_oldage_eu.csv). It is a multidisciplinary and cross-sectional database of individuals on health, economic status, etc. Individuals have been interviewed in 2007 and the same individuals six years later, in 2013 (if they were alive). Having said that, in our investigation mortality is being defined as passing away within 6 years. The data set covers 12 European countries with ```r nrow(share_raw)``` individuals asked. The data set has ```r ncol(share_raw)``` variables, but according to the excercise I am going to consider 6 variables. All the observation where even one of the variables had a missing value, those have been dropped, in addition to that, age has been limited to the range of 50 to 80, observations outside of this range have been dropped. By these means we now have ```r nrow(share)``` observation to investigate.

Summary statistics of variables considered

I would like to briefly summarize the basic information and statistics of all variables and to support that I have created a table containing the basic statistics, such as minimum and maximum values, mean, median, quartiles, standard deviation, skewness, and a few others, that are not subject of this assignment.

Variable *deceased*

It is a binary variable, which can take the value of 1 if the interviewee is deceased in the year of 2013, takes 0 otherwise. In our data set ```r mean(share$deceased)*100```% of the individuals passed away by 2013.

Variable *female*

It is also a binary variables, takes the value of 1 in case of the interviewee being female and takes 0 otherwise. ```r mean(share$female)*100```% of interviewees are female in our data set.

Variable *age*

The age variable represents the age of the interviewee in 2007, which we have limited to 50 to 80. The average is ```r mean(share$age)``` in the data set. Age is given in years rounded to one digit (considering months).

Variable *eduyears_mod*

This variable represents the years of education the individual had. Its range is 0 to 20 and has a normal-like distribution, the mean is ```r mean(share$eduyears_mod)```.

Variable *income10g*

This variable represents the household income, having 10 equal-sized groups within each of the 12 country, it is basically a categorical, numerical variable with a range of 1-10.

Variable *sports*

This particular variable represents teh frequency of sports or activities that are done vigorous by the individual asked. This categorical, numerical variable can take the values 1 to 4,

* 1 means doing sports more than once a week,

* 2 means doing sports once a week,

* 3 means doing sports one to three times a month and

* 4 means doing sports hardly ever, or never.

Please find some summary statistics of the variables introduced above.

```{r Q1_basic_statistics}
# Basic statistics of variables
# pander(basicStats(share[,1:6]))
pander(summary(share))

# Histogram of age variable
# ggplot(share, aes(age)) + geom_histogram()
# ggplot(share, aes(eduyears_mod)) + geom_histogram()
# ggplot(share, aes(income10g)) + geom_histogram()
# ggplot(share, aes(sports)) + geom_histogram()
```

### <span style="color:blue"> 2. Do exploratoty analysis: Create binary variables from the sports variable. Describe these variables in your dataset. Drop observations that have missing value for either.

I have briefly described the variables above, now having a deeper analysis, please find a frequency table below of the *sports* variable not considering the *deceased* variable. In our data set ```r mean(share$deceased)*100```% of the individuals passed away by 2013. In the year of 2007, 7961 interviewee reported that they did sports more than once a week, 3228 individuals reported that they did sports once a week, we have 2209 records saying they did sports one to three times a months and 8450 people reported they did sports hardly ever or never.

```{r Q2_frequency_table}
# Frequency tables, deceased = died 
# freq(share$deceased)
freq(share$sports)
```


Please find a cross table below, already considering the *deceased* variable, and a few statements about the results of the cross table:

* 62.74% of people who are alive in 2013, did sports at least one to three times a month, but a very high proportion, 37.26% did not do sports regularly. 
* 38.07% of people who are already passed away, did sport at least one to three times a month, and a higher proportion, 61.93% of them did not do sports regularly.
* People who did sport hadrly ever or never, 90.82% of them was alive in 2013.
* People who did sport at least one to three times a month or more frequent, `r (7700+3123+2098)/(7700+3123+2098+261+105+111)*100`% of them were alive in 2013.
* The difference in mortality rate is difference of the two above mentioned values, the difference is `r (7700+3123+2098)/(7700+3123+2098+261+105+111)*100-90.82` percentage points between who did sports at least one to three times a months and those who hardly ever or neved did sports.

```{r Q1_crosstable}
pander(CrossTable(share$deceased, share$sports))
```

#### Creating binary variables of *sports* variable

In the next exercise we are going to estimate linear probability models, they are regressions with binary dependent variables. The easiest way to work with binary variables if they are 0 or 1, so I have created 4 binary variables, *sports_1, sports_2, sports_3, sports_4*, they can take the values of 0 or 1. Each of these values indicates whether the individual falled into that category, 1 is taken if yes, 0 otherwise. This way the mean of the variable will be also be the probability that it is 1. I have created the below table for making interpretation easier, it shows the extent of people being alive or deceased in each *sports* category.

Deceased variable / Sports variable | sprots_1 | sports_2 | sports_3 | sports_4
------------- | ------------- | ------------- | ------------- | -------------
alive in 2013 | 96.72% | 96.75% | 94.98% | 90.82%
passed away by 2013 | 3.28%  | 3.25% | 5.02% | 9.18%

```{r Q2_binary_variables, results='hide'}
# The sports variables is a categorical variables on its own, but to be able to work with, binary variables have to be created from this categorical variable.

# Creating binary variables
share$sports_1 <- as.numeric(share$sports == 1)
share$sports_2 <- as.numeric(share$sports == 2)
share$sports_3 <- as.numeric(share$sports == 3)
share$sports_4 <- as.numeric(share$sports == 4)

# Summary statistics of sports binary variables
# summary(share$sports_1, digits = 2)
# summary(share$sports_2, digits = 2)
# summary(share$sports_3, digits = 2)
# summary(share$sports_4, digits = 2)
# summary(c(share$sports_1,share$sports_2,share$sports_3), digits = 2)
# table(share$sports_1, share$sports_2, share$sports_3, share$sports_4)
```

### <span style="color:blue"> 3. Estimate a linear probability model (LPM) of mortality on sports. Report and interpret the results.

A linear probability model with mortality as the dependent variable and *sports_1, sports_2, sports_3, sports_4* as explanatory variables will show us the differences in mortality rate depending on the frequency of doing sports. In this linear probability model our reference category is *sports_4* (doing sports hardly ever or never).

```{r Q3_LPM_model}
# LPM: linear probability model
lpm1 <- lm(deceased ~ sports_1 + sports_2 + sports_3, data=share)
# lpm2 <- lm(deceased ~ drinks_never + drinks_everyday, data=share)

coeftest(lpm1, vcov=sandwich)
# coeftest(lpm2, vcov=sandwich)
```

The coefficients are significant at 0.1%, meaning that we can be 99.9% confident that these coefficients are not zero in our data set. The intercept means that 9.18% of poeple who did sports hardly ever or never were deceased by 2013. As it is the reference category, in the following I am comparing the different probabilities to this category. Those who reported doing sports more than once a week had a 5.9 percentage points lower probability of being deceased within the 6 years between 2007 and 2013, compared to those who hardly ever or never did sports. The same applies to those who fell in sports_2 category, they had a 5.9 percentage points lower probability of being deceased within 6 years. Those that reported to do sports one to three times a month had a 4.2 percentage points lower probability of passing away within that 6 years, compared to individuals in reference category *sports_4*.

The following plot was created by the stargazer package, it does tell that same values as the coeftest function above, with some additional information and some differences. R-squared means, how much of the variance in the dependent variable was captured by the regression, but in case of probability models **we don't interpret R-squared**. The difference that I wanted to highlight is that according to stargazer the coefficients are significant at 1%, while coeftest function reported them being significant at 0.1%.

```{r Q3_stargazer_LPM1, results='asis', results='hide'}
# stargazer(list(lpm1), digits=3, type="html", out="sports_mortality_1.html")
```

```{r Q3_stargazer test}
stargazer(list(lpm1), digits=3, type="text", omit.stat=c("LL","ser","f", "aic"))
```

```{r Q3_shiny_LPM1}
# shiny::includeHTML('./sports_mortality_1.html')
```

The following scatterplot has a regression line plotted on it, showing that the *deceased* variable can take only two values, 0 and 1, and the *sports* variable can take the already described for values, 1 to 4. The regression line seems to be flat, but if we compared the probabilities of being deceased for *sports_1* and *sports_4* to each other, we would see that 5.9 percentage points are a 64% difference.

```{r Q3_scatterplot}
# scatterplot and regression line with all sports binary variables
ggplot(data = share, aes(x=sports, y=deceased)) +
  geom_point(colour="orange") +
  geom_smooth(method="lm", colour="navy") +
  ggtitle(labs(title = "Q3 Scatterplot with a regression line", x = "sports categorical variable", y = "mortality rate"))
```

The following chart shows the same regression line, but not showing the whole range of *deceased* variable. On this chart is is easier to read the general relation that we have already discussed above.

```{r Q3_regression_line}
# only  regression line with smoking
ggplot(data = share, aes(x=sports, y=deceased)) +
  geom_smooth(method="lm", colour="navy") +
  ggtitle(labs(title = "Q3 Regression line", x = "sports categorical variable", y = "mortality rate"))
```

### <span style="color:blue"> 4. If you are interested in the causal effect of doing sports on mortality, would you want to control for some of the other variables in the dataset to get closer to the causal effect you are after? Would that controlling get you the causal effect you are after? 

If we were interested in the causal effect, it would not be enough just to investigate the frequency of doing sports. To investigate the possible causal effects of doing sports is quite difficult, if not impossible. If one were looking for a causal effect, in theory all the other variables that might influence the outcome of our investigation, should be controlled for. It is called ceteris paribus, which is a theoretical therm for economists to say that all the variables are held constant, except the one under investigation.

So the possible way to get closer to causal effect is to control for reasonable variables, confounders, that we have data about. We have information about age, gender, household income and number of years spent in education, by countries. These would be the variables that I would consider as possible confounders, assuming that these variables might influence the possible effect of doing sports.

### <span style="color:blue"> 5. Control for those variables in another LPM, interpret its results on sports, and compare those to the previous regression estimates. Discuss the differences and similarities.

I am going to run LPM regressions with mortality rate as dependent variable, controlling for *sports_1, sports_2, sports_3, sports_4* variables, and controlling for the other demographic variables mentioned, one LPM regression for each.

#### Checking *age* as a confounder

By regressing a non-parametric, a linear, a quadtratic and a cubic LPM regression on *age* we can see that the cubic regression is very close to the non-parametric regression, it provides a good fit. By eyeballing we can see a general positive relationship, so we can say that on average, the older an individual, the higher the probability of passing away within 6 years.

```{r Q5_LPM_age_loess, results='asis', results='hide', eval=FALSE}
# HANDLING CONFOUNDERS IN LPM to validate our estimation

# Functional for age
# watch out: loess takes a lot of time to run
ggplot(data = share, aes(x=age, y=deceased)) +
  geom_smooth(method="loess", aes(colour="loess")) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  ggtitle(labs(title = "Q4 LPM for age variable", x = "age variable [years]", y = "mortality rate")) +
  scale_colour_manual(name="Legend",values=c("#66CC00","#00BFC4","#F8766D", "black"))

# same without loess
ggplot(data = share, aes(x=age, y=deceased)) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D", "black"))
```

```{r Q5_LPM_age_non-parametric}
# alternative nonparametric: fraction deceased by single years of age
# first create single years of age
share$agey <- round(share$age)
byage <- aggregate(share$deceased, list(agey=share$agey), mean)
ggplot(data = share, aes(x=age, y=deceased)) +
  geom_line(data = byage, aes(x=agey, y=x, colour="non_parametric"), size=3) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D", "black", "green")) +
  ggtitle(labs(title = "Q4 LPM for age variable", x = "age variable [years]", y = "mortality rate"))
```

#### Checking *eduyears_mod* as a confounder

By regressing a non-parametric, a linear, a quadtratic and a cubic LPM regression on *eduyears_mod* we can see that the cubic regression is very close to the non-parametric regression, it provides a good fit. As we can see the regression has different segments. In the range of 3 to 17 years spent in education we can see a general negative relationship, thus we can say that on average the more years spent in education, the lower the probability of passing away within 6 years. Considering those who spent less than 3 or more than 17-18 years in education we can say that on average, the more years spent in average, the higher the probability is of passing away in 6 years. One side note could be thta considering those who spent less than 3 years in education might be irrelevant.

```{r Q5_LPM_education_loess, results='asis', results='hide', eval=FALSE}
# Functional for for education
# watch out: loess takes a lot of time to run
ggplot(data = share, aes(x=eduyears_mod, y=deceased)) +
  geom_smooth(method="loess", aes(colour="loess")) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D", "black", "green"))

# same without loess
ggplot(data = share, aes(x=eduyears_mod, y=deceased)) +
  geom_smooth(method="lm", colour="orange") +
  geom_smooth(method="lm", formula=y~poly(x,3), colour="navy")
```

```{r Q5_LPM_education_non-parametric}
# alternative nonparametric: fraction deceased by single years of education
byedu <- aggregate(share$deceased, list(edy=share$eduyears_mod), mean)
ggplot(data = share, aes(x=eduyears_mod, y=deceased)) +
  geom_line(data = byedu, aes(x=edy, y=x, colour="non-parametric"), size=3) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  ggtitle(labs(title = "Q4 LPM for education variable", x = "years spent in education [years]", y = "mortality rate")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D", "black", "green"))
```

#### Checking *income10g* as a confounder

By regressing a non-parametric, a linear, a quadtratic and a cubic LPM regression on *income10g* categorical variable, we can see that the cubic regression is very close to the non-parametric regression, it is well fitted. Those who fell in income categories of 3 to 9, we can see a general negative relationship, on average the higher the household income is, the lower the probability of passing away within 6 years. For those who fell in the frist two or last category, the relation is the positive, the higher the income is, the higher the probability of being deceased in 6 years, on average.

```{r Q5_LPM_for_income_loess, results='asis', results='hide', eval=FALSE}
# Functional for for income group
# watch out: loess takes a lot of time to run
ggplot(data = share, aes(x=income10g, y=deceased)) +
  geom_smooth(method="loess", colour="black") +
  geom_smooth(method="lm", colour="orange") +
  geom_smooth(method="lm", formula=y~poly(x,3), colour="navy") 
# same without loess
ggplot(data = share, aes(x=income10g, y=deceased)) +
  geom_smooth(method="lm", colour="orange") +
  geom_smooth(method="lm", formula=y~poly(x,3), colour="navy")
```

```{r Q5_LPM_for_income_non-parametric}
# alternative nonparametric:  fraction deceased by 10 income groups
byinc <- aggregate(share$deceased, list(inc=share$income10g), mean)
ggplot(data = share, aes(x=income10g, y=deceased)) +
  geom_line(data = byinc, aes(x=inc, y=x, colour="non-parametric"), size=3) +
  geom_smooth(method="lm", aes(colour="linear")) +
  geom_smooth(method="lm", formula=y~poly(x,2), aes(colour="quadratic")) +
  geom_smooth(method="lm", formula=y~poly(x,3), aes(colour="cubic")) +
  ggtitle(labs(title = "Q4 LPM for income variable", x = "income variable", y = "mortality rate")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D", "black", "green"))
```

#### Estimating LPMs with possible confounders

I have estimated three regressions, (1) is the regression from question 3, considering only the *sports* categorical variables, these results have already been interpreted. The (2) regression is considering the confounders in a linear model, (3) regression considers the same confounders but in a cucbic model, more precisely:

* (1) *deceased* on *sports_1, sports_2, sports_3*, *sports_4* as a reference category, in a linear model,

* (2) *deceased* on *sports_1, sports_2, sports_3, female, age_new, eduyears_mod, income10g_new*, *sports_4* as a reference category, in a linear model,

* (3) *deceased* on *sports_1, sports_2, sports_3, female, age_new, eduyears_mod, income10g_new*, *sports_4* as a reference category, in a cubic model.

*age_new* variable has been introduced to make the interpretation easier, I have substracted 50 from *age* variable, so when regressing an LPM model, interpreting the intercept will be easier. I have done the same procedure to *income10g* variable, but have substracted only 1.

Looking at *sports_1, sports_2, sports_3* variables we can clearly see that considering the confounders made all the coefficients lower. All the *sports* variables are significant at 0.1%, according to the results of coeftest function. Please consider the coming analysis with keeping the following in mind, when comparing the coefficients we assume that the individuals being compared had the same age and gender, spent the same number of years in education, fell into the same household income category, and we are comparing them to the reference category, to individuals who did sports hardly ever or never. With other words, in this LPM we are looking for average difference in mortality rate corresponding to differences in sporting habits with all the other confounders being the same and with *sports_4* as reference category.

According to regressions (2) and (3), those who did sports more than once a week, had a 4.4 and 4.3 percetnage points lower probability of passing away within 6 year, respectively. It is very similar to indivudals falling in *sports_2* catogery. Those who did sports one to three times a months had a 3.7 and 3.5 percentage points lower probability of being deceased in 6 years. The intercept refers to *sports_4* reference category representing people that did sports hardly ever or never, they had a 6.6 and 10.3 percentage points higher mortality rate within 6 years. As there is no significant difference between regression (2) and (3), including polinomial components did not make significant difference, except in case of *sports_4* category, considering which one is easier to interpret, I would choose regression (2) for further analysis and interpretation.

```{r Q5_LPM_model, results='hide'}
share$age_new <- share$age-50 # otherwise intercept would be more difficult to interpret, it would be a high positive number and misleading the interpretation
share$income10g_new <- share$income10g-1

lpm2 <- lm(deceased ~ sports_1 +sports_2 + sports_3 + female + age_new + eduyears_mod + income10g_new, data=share)
lpm3 <- lm(deceased ~ sports_1 +sports_2 + sports_3 + female + poly(age_new,3) + poly(eduyears_mod,3) + poly(income10g_new,3), data=share)
```

```{r Q5_coeftest}
coeftest(lpm2, vcov=sandwich)
coeftest(lpm3, vcov=sandwich)
```

The exercise was asking to interpret the results on sport. I have interpreted the other variables as well, please find it in the appendix.

After interpreting the results we can conclude that adding the confounders provided a better estimation about the possible positive effects of doing sports, but it is still not enough to consider causal effects. We can only state those who frequently do sports, are less likely to pass away on average within 6 years. There are much more variables that we could consider, such as sleeping habits, chronical deceases in the familiy, eating habits, etc. We have also uncovered that having polinomial components in the regression did not provide significant difference.

The following plot was created by the stargazer package, I have added it to provide a nicer output for the results.

```{r Q5_stargazer}
# stargazer(list(lpm1, lpm2, lpm3), digits=3, type="html",out="sports_mortality_2.html")
stargazer(list(lpm1, lpm2, lpm3), digits=3, type="text", omit.stat=c("LL","ser","f", "aic"))
```

```{r Q5_shiny_LPM1}
# shiny::includeHTML('./sports_mortality_2.html')
```

### <span style="color:blue"> 6. Re-do exercises 3 & 5 using logit. Calculate and interpret the marginal differences of the sports variables. Discuss the differences and similarities to the LPM results.

For prediction Logit and Probit models are used in general. The reason for it is that predicted probabilities can be more than 1 or less than 0 and therefore we cannot use them for predictions. There are models that have the feature of providing a probability that is always strictly between 0 and 1, they are called logit and probit. They are basically non-linear regressions with binary variables. The slope of these models are not constant and are practically meaningless, I won't interpret them. In case of these models we are investiagting the marginal differences, which show us the average differences in mortality rate corresponding to a one unit difference in one of the explanatory variables, while the other variables are kept constant. With other words it shows the average marginal effect of a one unit change in my explanatory variable, while the other variables are kept fixed.

#### Logit and Probit models compared to LPM with linear components - regression (1)

I am going to compare the results of Logit and Probit models to the results of question 3 and 5,

I am starting the comparison with results of question 3 where we did not control for any variables other than sport variables. All the marginal differences are bsaically the same in the Probit and Logit models, but they are not equal to *sports* variables we saw in regression (1). We are comparing now individuals to the reference category, to those who hardly ever or never did sport.

Those who did sports more than once a week had a 5.1 (in both Logit and Probit) percentage points lower probability of passing away within six years. In LPM this value was 5.9 percentage points. Those who did sports once a week had lower percentage points compared to the LPM model, according to Logit and Probit they had a 4.3-4.4 percentage points lower likelihood of being deceased in 6 years. For the record, it was 5.9 percentage points in the LPM model. Those who did sports one to three times a month were 2.8-2.9 percentage points less likely to die within 6 years. In the regression (1) it was 4.2 percentage points.

```{r Q6_logit_computation_for_regression_1}
# LOGIT

logitcoeffs_q3 <- glm(deceased ~ sports_1 +sports_2 + sports_3, data=share, family='binomial')
# glm function for generalization, family binomial - it means exponential function will be used
logitmarg_q3 <- logitmfx(formula = deceased ~ sports_1 +sports_2 + sports_3, data=share, atmean=FALSE)
# logitmfx - computing marginal effect
# summary(logitcoeffs) # don't interpret these coefficients

print(logitmarg_q3)
# marginal effects - on average in my sample what is the marginal effect of a small increase in my explinatory variable
# it is about the marginal change on average; all the other variables kept fix, the avg marginal effect of unit increase is this much ...
# logit estimates something similar to what the simple linear regression model estimates
# if i want to do prediction, probit and logit performs better
```

```{r Q6_probit_computation_for_regression_1}
# PROBIT

probitcoeffs_q3 <- glm(deceased ~ sports_1 +sports_2 + sports_3, data=share, family='gaussian')
probitmarg_q3 <- probitmfx(formula = deceased ~ sports_1 +sports_2 + sports_3, data=share, atmean=FALSE)
# summary(probitcoeffs)

print(probitmarg_q3)
```

Logit and Probit models provided lower probabilities than linear LPM.

#### Logit and Probit models compared to LPM with linear components - regression (2)

All the marginal differences are essentially the same in the Probit and Logit models, but they are not equal to *sports* variables we saw in regression (2). We are comparing now individuals with different sporting habits to those that hardly ever or neved did sports, but had the same gender, age, household income category and numbers of years spent in education. Those who did sports more than once a week had a 3.7 percentage points lower probability of passing away within six years. In LPM this value was 4.5 percentage points. Those who did sports once a week had also lower percentage points compared to the LPM model, according to Logit and Probit they had a 3.4 percentage points lower likelihood of being deceased in 6 years. For the record, it was 4.5 percentage points in the LPM model. Those who did sports one to three times a month were 2.5 percentage points less likely to die within 6 years.

Please find the marginal differences of Logit model below:

```{r Q6_logit_computation_for_regression_2}
# LOGIT

logitcoeffs_q5 <- glm(deceased ~ sports_1 +sports_2 + sports_3 + female + age_new +eduyears_mod + income10g_new, data=share, family='binomial')

logitmarg_q5 <- logitmfx(formula = deceased ~ sports_1 +sports_2 + sports_3 + female + age_new +eduyears_mod + income10g_new, data=share, atmean=FALSE)

print(logitmarg_q5)
```

Please find the marginal differences of the Probit model below:

```{r Q6_probit_computation_for_regression_2}
# PROBIT

probitcoeffs_q5 <- glm(deceased ~ sports_1 +sports_2 + sports_3 + female + age_new +eduyears_mod + income10g_new, data=share, family='gaussian')
probitmarg_q5 <- probitmfx(formula = deceased ~ sports_1 +sports_2 + sports_3 + female + age_new +eduyears_mod + income10g_new, data=share, atmean=FALSE)
# summary(probitcoeffs)

print(probitmarg_q5)
```

To compare the results of coefficients of both Probit and Logit models to the results of LPM regression (1) and (2) I have created the below table. In column one and two you can see the coefficient of regression (1) and (2) from the previous exercises. The next four columns represents the coefficients of the Logit models and Probit models. We can see that the coefficients provided by the Logit model are approximately 20 times more than the results of LPM. These values are meaningless, we cannot interpret them. The Probit models provided the same coefficients as the LPM regressions.

```{r Q6_stargazer_logit_probit}
stargazer(list(lpm1, lpm2, logitcoeffs_q3, probitcoeffs_q3, logitcoeffs_q5, probitcoeffs_q5), digits=3, type="text", omit.stat=c("LL","ser","f", "aic"))
```

As I have mentioned previously that the difference between the coefficients provided by the LPM regression and the Logit model is significant. The following plot compares the prediction of these two (both LPM and Logit considering the confounders). The magenta line is 45 degree and is just supporting the interpretation.

```{r Q6_comparing_LPM_logit, results='hide'}
share$pred_lpm <- predict.lm(lpm2)
share$pred_logit <- predict.glm(logitcoeffs_q5, type="response")

ggplot(data = share, aes(x=pred_logit, y=pred_lpm)) +
  geom_line(aes(x=pred_logit, y=pred_logit, colour="support line")) +
  geom_point(aes(colour="prediction")) +
  ggtitle(labs(title = "Q6 Comparing logit(x axis) to LPM(y axis)", x = "prediction of logit model", y = "prediction of LPM model")) +
  scale_colour_manual(name="Legend",values=c("#00BFC4","#F8766D"))
# comparing the prediction of logit (x axis) to the linear model (y axis)
# orange is just a 45 degree line to help interpretation
```

#### Logit model in a stylized example

In this stylized example I would like to show that Logit (and Probit) model may produce a better fit than LPM. In this example I am only investigating the mortality probability on the *age* variable. Four different regressions are considered on the following plot, a non-parametric, a LPM linear in age, an LPM cubic in age and Logit quadratic in age. The plot clearly show us that a Logit can give better predictions as a linear LPM regression.

```{r Q6_age_with_LPM_logit, results='hide'}
share$fit_lpm2 <- fitted.values(lpm2)
share$fit_logit <- fitted.values(logitcoeffs_q5)

# qplot(share$fit_lpm2)
# qplot(share$fit_logit)

lpme<-sum((share$fit_lpm2 - share$deceased)^2)
logite<-sum((share$fit_logit - share$deceased)^2)

ggplot (share) +
  ggtitle("Predicted Probabilities") + xlab("Age") + ylab("Predicted mortality rate") +
  geom_line(data = byage, aes(x=agey, y=x, colour="Non-parametric"), size=2) +
  geom_smooth(data=share, method="lm", aes(x=age, y=fit_lpm2, colour="Linear LPM")) +
  geom_smooth(data=share, method="lm", formula=y~poly(x,2), aes(x=age, y=fit_logit, colour="Logit")) +
  geom_smooth(data=share, method="lm", formula=y~poly(x,3), aes(x=age, y=deceased, colour="LPM")) +
  scale_colour_manual(name="Legend",values=c("#66CC00","#FFFF33","#F8766D","#00BFC4"))

# ggplot (share) +
#  ggtitle("Predicted Probabilities") + xlab("Age") + ylab("Predicted mortality rate") +
#  geom_line(data = byage, aes(x=agey, y=x, colour="Non-parametric"), size=2) +
#  geom_smooth(data=share, method="lm", aes(x=age, y=fit_lpm2, colour="Linear LPM")) +
#  geom_smooth(data=share, method="lm", aes(x=age, y=fit_logit, colour="Logit")) +
#  geom_smooth(data=share, method="glm", formula=y~poly(x,3), aes(x=age, y=fit_lpm2, colour="LPM")) +
#  scale_colour_manual(name="Legend",values=c("#66CC00","#FFFF33","#F8766D","#00BFC4"))
```

#### Final conclusion

We could not prove and did not try to prove that there is a causal relationship between mortality and doing sports, but we were targeting to find patterns and possible confounders to get closer to a general relationship between mortality and doing sports. We could show that on average, people who do sports more often are expected to pass away later.

### <span style="color:blue"> 7. Appendix

#### Question 5 - further interpreting other variables

In regression (2) all the confounders are significant at 0.1%. The coefficient of female variable is -3.7, meaning that females had a 3.7 percentage point lower mortality rate on average compared to men with the same age, having spent the same number of years in education, being in the same household income category and having the same sporting frequency. Age has a positive slope, meaninig the on average, people had a 0.5 percentage points higher probability of passing away within 6 years compared to those who were 1 year younger, had the same gender, spent the same number of years in education and had the same sporting frequency.

Looking at *income10g_new* variable we can state that people who fell in a certain household income category had a 0.1 percenatge points lower mortality rate on average compared to those who fell into a 1 unit lower income category, had the same gender, spent equal years in education, had the same age and sporting frequency. *eduyears_mod* variable had a negative slope as well, but a very low one. People who spent the same number of years in education were 0.1 percentage points less likely to be deceased within 6 yeas, on average, compared to those who spent 1 year less in education, but had the same age, gender, sporting habits and fell into the same household income category.