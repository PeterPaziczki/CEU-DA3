---
title: "Data Analysis 3 - Pattern Discovery and Regression Analysis 2017/2018 Fall"
author:
- name: Peter Paziczki
- name: Imre Boda
- name: Balazs Zankay
date: '2017 november 22'
output:
  html_document: default
  pdf_document: default
  word_document: default
---

<style>
body {
text-align: justify}
</style>

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r load}
library(data.table)
library(ggplot2)
#library(haven)
library(lspline)

# install.packages("arm")
#library(arm)
# install.packages("readr")
#library(readr)
# install.packages("lmtest")
#library(lmtest)
# install.packages("splines")
#library(splines)

#presentation parameters
theme_update(plot.title = element_text(hjust = 0.5, size = 12))

#Reading input files
GDP <- fread('GDP_per_capita_2014-Data.csv', encoding = 'UTF-8')
Life <- fread('Life_expectancy_at_birth_2014-Data.csv', encoding = 'UTF-8')
#Reading file to compare entries against country list
CountryList <- read.csv('https://www.searchify.ca/wp-content/uploads/2016/09/country-keyword-list.csv',fileEncoding="UTF-16LE")
```

# DA3 Assignment #2

## Problem2.1


### <span style="color:blue">Download cross-country data on life expectancy and GDP per capita. 
- “GDP per capita, PPP (constant)” and “Life expectancy at birth (total)”
</span>

We have downloaded both the above mentioned indicators for 2014, storing them in two separate csv files, GDP_per_capita_2014-Data.csv and Life_expectancy_at_birth_2014-Data.csv.

### <span style="color:blue"> Q1. 1. Delete unnecessary columns and save a csv file with three columns only: country name, life expectancy and GDP per capita. Keep countries with non-missing values for life expectancy and GDP per capita. Document what you do.
</span>

```{r Q1_basic}
# Joining the two tables by Country name column.

# Settin the ON clause as keys of the tables:
setkey(GDP, `Country Name`)
setkey(Life, `Country Name`)
# Inner joining the two tables, DT syntax
Data_raw <- Life[GDP, nomatch=0]

# Dropping the unnecessary columns, keeping only country name, life expectancy and GDP per capita columns
Data <- Data_raw[, -c(1,2,4,6:8)]
# renaming country name, life expectancy and GDP per capita columns for easier usage
names(Data) <- c("country", "life", "GDP")

# Dropping the obsevórvations where there is no country, and dropping countries where life expectancy and/or GPD are missing
Data <- Data[country!="",]
Data <- Data[life!="..", ]
Data <- Data[GDP!="..", ]
n <- Data_raw[`Country Name` != "",.N] # number of countries in the original data set

# Both life expectancy and GDP per capita are character variables, they need to be converted into numeric variables.
Data <- Data[, lapply(.SD, as.numeric), by = country, .SDcols = c('life', 'GDP')]

# Shaping data
Data [, "GDP"] <- Data [, .(GDP = GDP/1000)]
# Taking the log of GDP per capita and sroting it in logGDP column.
Data [, logGDP := log(GDP)]
```

We have loaded both csv files and merged them by using a join like command. By using this special command we could make sure that relevant columns (Life expectancy and GDP) from the two tables were merged by matching the country name column, so the right values have been paired. The countries with missing life expectancy and/or GDP values have been dropped. We have `r Data[,.N]` entries of the `r n` left to analyze. Other unnecessary columns have also been dropped, such as Series Name, Series Code, Country Code. We have renamed the remaining columns as *country*, *life* and *GDP*, so it will be easier to type and use them. Finally we have divided GDP by 1000 and took the log of it.
We also downloaded a csv file containing the list of countries in the world for our next step, which is Data Cleaning.

#### Data Cleaning
We have aggregated lines in the data, that we have to remove as we only care about countries in the analysis.
We do this cleaning in two steps: first we check entries where the "country" is not inlcuded in the list of downloaded countries.
Then we manually check all of them, and collect those that are really not countries. (In the previous step some get classified as "non country" due to different writing in Data Bank and in the downloaded country list csv file). Once we have this collection of real non-countries, we remove all from the Data.
```{r Q1_Cleaning}
SuspectedGarbage <- Data[!country %in% CountryList[,1]]            
Garbage <- SuspectedGarbage [ -c(1,2,4,5,6,7,8,11:15,20,21,28,29,32,38,39,40,41,50,51,52,61,64,65,66,68,71,75,76,77,80),]
#let's check that we put non-countries to garbage and we kept only countries
Garbage [, country]
SuspectedGarbage [ c(1,2,4,5,6,7,8,11:15,20,21,28,29,32,38,39,40,41,50,51,52,61,64,65,66,68,71,75,76,77,80),country]
#Removing all non country entries from the database and only keep countries
Data <- Data[!country %in% Garbage [,country]]
```


Please find summary statistics of life variable below:
```{r Q1_LifeSummary}
summary(Data[,life])
ggplot (Data, aes(life)) + geom_histogram(binwidth = 1) + ggtitle(labs(title = "Q1. Histogram of life variable", x = "Life expectancy at birth [years]"))
Data [life == Data [,min(life)],]
Data [life == Data [,max(life)],]
```

Life expactancy is the highest in Hong Kong SAR, China, and the lowest in the Central African Republic.

Please find summary statistics of GDP variable below:
```{r Q1_GDPSummary}
summary(Data[,GDP])
ggplot (Data, aes(GDP)) + geom_histogram ( binwidth=6) + ggtitle(labs(title = "Q1. Histogram of GDP variable", x = "GDP per capita [1000 USD]"))
Data [GDP == Data [,min(GDP)],]
Data [GDP == Data [,max(GDP)],]
```

GDP follows a log normal like distribution with a long right tail, it is tipically a clear sign that we are better to take the logarithm of the variable. GDP is the highest in Macao SAR, China, and the lowest in the Central African Republic. 

Please find summary statistics of logGDP variable below:
```{r Q1_logGDP}
summary(Data[,logGDP])
ggplot(Data, aes(logGDP)) + geom_histogram (binwidth=0.5) + ggtitle(labs(title = "Q1. Histogram of logGDP variable", x = "Log GDP per capita [1000 USD]"))
```


### <span style="color:blue"> Q2. Estimate a lowess regression of life expectancy on ln gdp per capita. Estimate a linear regression of life expectancy on GDP per capita that best captures the nonlinearity you found (life expectancy on a piecewise linear spline or a polynomial in the explanatory variable). Argue for your choice. Report the coefficient estimates as well as their confidence interval, interpret and visualize the results.
</span>

We took the log of GDP per capita, as we are interested in relative % changes instead of absolute terms. Lowess produces a smooth curve, that allows us to capture non-linear patterns. (The con of lowess is thata it does not provide a formula to describe the relationship between life expectancy and GDP.) What lowess does is that it provides an expected y value for every x values and for x values in-between resulting in a graph that we can use in qualitative analysis.

In this particular case we have log GDP per capita on the x axis and life expectancy on the y axis.

```{r Q2_lowessGraph}
# Estimating a lowess non-parametric regression on the data set.
reg_loess <- loess (life ~ logGDP, Data)
summary (reg_loess)
ggplot(data = Data, aes(x=logGDP, y=life)) +
  geom_point(size=1.5) +
  ggtitle ("Q2. Lowess regression") + labs(x="GDP per capita, PPP [USD]",y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=logGDP,y=predict(reg_loess)),colour="blue",size = 1)
```

We can see that the graph has a positive slope in general. At the right end of the curve it gets a little flatter, but remains still positive. So we can assumte that there is some sort of positive correlation between these two variables.


In the following sections we go deeper in understanding our data.

First, let's have a scatterplot.
```{r Q2_scatterplots}
# Creating a scatterplot to have a quick look at the data we have:
ggplot(Data, aes(x = GDP, y = life)) + geom_point() +
  ggtitle ("Q2. GDP - Life expectancy") + labs(x="GDP per capita, PPP [USD]",y="Life expectancy at birth [years]") 
ggplot(Data, aes(x = logGDP, y = life)) + geom_point () +
  ggtitle ("Q2. Log GDP - Life expectancy") + labs(x="Log GDP per capita, PPP [USD]",y="Life expectancy at birth [years]")
```

The above graphs show that we are probably on the right track when trying to find relationship between log GDP and Life Expectancy, rather than GDP and Life Expectancy.


#### Level - Log Linear Regression
``` {r Q2_evLogRegression}
reg_levlog <- lm(life ~ logGDP, data=Data)
summary (reg_levlog)
#ggplot (Data, aes (x=logGDP, y = life )) + geom_point (size = 1.5) + 
#ggtitle("Q2. Level - Log linear regression") + labs(x = "Log GDP per capita [1000 USD]", y = "Life Expectancy [year]") + 
#geom_smooth (method = "lm")
ggplot (Data, aes (x=logGDP, y = life )) + geom_point (size = 1.5) + 
ggtitle("Q2. Level - Log linear Regression") + labs(x = "Log GDP per capita [1000 USD]", y = "Life Expectancy [year]") + 
geom_line(data=Data,aes(x=logGDP,y=predict(reg_levlog)),colour="blue",size = 1)

```

We seem to have rather small residual errors and pretty good R-square, confirming the relationship between logGDP and Life Expectancy.
The coefficients: at 0 logGDP the Expected Life is ~58 years, and at every percent in GDP change results in ~5.5/100 year change in average life expectancy.

#### Log - Log regression
``` {r Q2_Log-Log}
# LOG-LOG REGRESSION
reg_loglog <- lm(log(life) ~ logGDP, data=Data)
summary(reg_loglog)
ggplot(data = Data, aes(x=logGDP, y=log(life))) +  geom_point(size=1.5) +
  ggtitle ("Q2. Log-Log linear regression") +  
  labs(x="Log - GDP per capita, PPP [USD]", y="Log - Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=logGDP,y=predict(reg_loglog)),colour="blue",size = 1)
```  

!!! ÉRTÉKELÉST IDE


#### Lev - Lev regression

Though the scatterplots above suggested that the level - level linear correlation probaly is not the best model, just for the sake of completeness let's see a level-level linear regression model.
``` {r Q2_Lev-Lev}
# LOG-LOG REGRESSION
reg_levlev <- lm(life ~ GDP, data=Data)
summary(reg_levlev)
ggplot(data = Data, aes(x=GDP, y=life)) +  geom_point(size=1.5) +
  ggtitle ("Q2. Level - Level linear regression") +  
  labs(x="GDP per capita, PPP [USD]", y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=GDP,y=predict(reg_levlev)),colour="blue",size = 1)
```  

Both R-square and the plot confirms our assumption that there are much better models that the simple level - level linear regression.

#### Level - Level Quadratic regression
``` {r Q2_QuadraticRegression}
# Estimating a quadratic regression on GDP and life expectancy:
Data[, GDP_sq := GDP^2]
reg_quad <- lm(life ~ GDP + GDP_sq, data=Data)
summary(reg_quad)

ggplot(Data, aes(x=GDP, y=life)) +  geom_point(size=1.5) + 
  ggtitle ("Q2. Quadratic regression") +  
  labs(x="GDP per capita, PPP [USD]", y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=GDP,y=predict(reg_quad)),colour="blue",size = 1)
```

R-square is lower and residual error is higher than before, indicating that it might not be as good model as the previous one is. Frankly, probably there is nothing to justify that life expectancy would peek at `r round (0.58 / 2 / 0.0039)` kUSD GDP per capita, and very rich countries would suffer. The reasonably small error rate to this model is likely attributable to the fact that we have very few very rich countries (hence, small number of observations on this end.)

#### Level - Level Cubic Regression
``` {r Q2_CubicRegression}
Data[,GDP_cu := logGDP^3]
reg_cub <- lm(life ~ GDP + GDP_sq + GDP_cu, data=Data)
summary(reg_cub)

ggplot(data = Data, aes(x=GDP, y=life)) +
  geom_point(size=1.5) +
  ggtitle ("Q2. Cubic regression") +  
  labs(x="GDP per capita, PPP [USD]", y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=GDP,y=predict(reg_cub)),colour="blue",size = 1)
```

#### Piecewise Linear Spline

In the lowess diagram we see that something is changing at about x = 4. We select the knots accordingly and build the PLS model accordingly.
``` {r Q2_PLS}

# PIECEWISE LINEAR SPLINE IN LEVEL-LOG REGRESSION
knots <- c(4)
reg_PLS <- lm(life ~ lspline(logGDP,knots), data=Data)
summary (reg_PLS)
ggplot(data = Data, aes(x=logGDP, y=life)) +
  geom_point(size=1.5) +
  ggtitle ("Q2. Piecewise Linear Spline") +
  labs(x="Log GDP per capita, PPP [USD]", y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=logGDP,y=predict(reg_PLS)),colour="blue",size = 1) +
  geom_vline(xintercept =knots,colour="red") 
```

#### Choosing the best model and evaluating it
!!! BLA-BLA-BLA
```{r Q2_Model selection}

RSq <- data.table (c("Level - Log  :  ",
                     "Log - Log    :  ", 
                     "Level - Level:  ", 
                     "Quadratic    :  ", 
                     "Cubic        :  ", 
                     "PLS (spline) :  "), 
                   c(summary(reg_levlog)$r.squared,
                     summary(reg_loglog)$r.squared,
                     summary(reg_levlev)$r.squared,
                     summary(reg_quad)$r.squared,
                     summary(reg_cub)$r.squared,
                     summary(reg_PLS)$r.squared
                     ))
RSq
```
!!! Azt javaslom, hogy legyen a level-log. Nagyon jó az R-square-je, és magyarázható is.
!!! Meg kell indokolni hogy ezért választottuk ezt.
!!! Koefficiensek magyarázatát be kell másolni fentről.

#### Coefficient calculations, confidence interval
!!! Alább azt tételezem fel, hogy a level-log-t választottuk az előző paragrafusban
!!! Ide is kell BLA-BLA
``` {r Q2_Coefficients}
### x = logGDP, y = life

## easy way:
coef (reg_levlog)
confint (reg_levlog)

## manual
Beta_estimate <- cov (Data [,logGDP], Data[,life]) / var (Data [, logGDP]) # cov (x,y) / var (x)
Alpha_estimate <- Data [, mean(life)] - Beta_estimate * Data [,mean (logGDP)]  # mean (y) - beta * mean (x)
Beta_estimate
Alpha_estimate
# sd of Residual Error calculation in 3 ways
sdResidErr1 <- sd (Data [, life - (Alpha_estimate + Beta_estimate * logGDP)]) #R built in sd function
sdResidErr2 <- Data [, (sum((life - (Alpha_estimate + Beta_estimate * logGDP))^2) / (.N-1))^0.5] #formula, n-1
sdResidErr3 <- Data [, (sum((life - (Alpha_estimate + Beta_estimate * logGDP))^2) / (.N-2))^0.5] #formula, n-2

sdResidErr1
sdResidErr2
sdResidErr3
# interpretation: sdResidErr1 and sdResidErr2 are the same, but different from tha value that we saw in summary (reg_levlog) above. The reason is that summary (reg_levlog) calculated with df = N-2, as it was also mentioned in that printout. In the below confidence interval calculation we will use sdResidErr1
Conf95 <- c(Beta_estimate - 1.96 * sdResidErr1 / Data [,sd(logGDP)]  / (nrow (Data)-1) ^0.5, Beta_estimate + 1.96 * sdResidErr1 / Data [,sd(logGDP)]  / (nrow (Data) -1) ^0.5)
Conf95

```

Coefficients and Confidence interval at 95% have been calculated with in built R functions as well as with statistical formulas.
!!! BLA-BLA


!!! BLA-BLA: worth noting that Alpha and Beta estimates are the same that we got earlier, and examined when printed the summary of reg_levlog model.


### <span style="color:blue"> 3. Estimate a weighted regression (weight=population). Compare results to what we saw in class. 
</span>

In order to do it, first we need to enhance our data with population. For that we downloaded 2014 population data form the same "World Development Indicators" website.

``` {r Q3_Population}
## Download and clean population data
PopData <- fread ('Population_2014.csv')
str (PopData)
PopData <- PopData [PopData [[2]] == "SP.POP.TOTL"]   #filtering to Total Population only (where data in column 2 = "SP.POP.TOTL")
PopData <- PopData [, -c(1,2,4)]
names(PopData) <- c("Country", "Population")
PopData <- PopData [, .(Population = as.numeric(Population)), by = Country]
PopData <- PopData [!is.na(Population),]
str (PopData)

## Enhance Data with population info
setkey(PopData, `Country`)
setkey(Data, `country`)
Data2 <- Data[PopData, nomatch=0] #inner join DT syntax
str (Data2)
# sanity check (values)
Data2 [is.na(Population),]
Data2 [country == "Hungary",]
Data2 [country == "United States",]
# Cool, we got data filled in for all
```

``` {r Q3_weighted model}
reg_weight<- lm (life ~ logGDP, weight = Population, data = Data2)
summary (reg_weight)
ggplot (Data2, aes (x = logGDP, y = life, size = Population)) + 
  geom_point(shape = 21, fill = "#009E73", alpha = 0.5, show.legend = FALSE) + 
  scale_size_continuous(range = c(1, 15)) +
  ggtitle ("Q3. Weighted Lev-Log Regression") +
  labs(x="Log GDP per capita, PPP [USD]", y="Life expectancy at birth [years]") +
  geom_line(data=Data,aes(x=logGDP,y=predict(reg_weight)),colour="blue",size = 1)
  
```


!Bla-Bla. Ide kell majd az, hogy nagyon hasonlít az unweighted-re, mert a nagy population országok ott vannak (az eredeti) regressziós egyenes közelében


##<span style="color:blue"> Problem 2.2

Download hotels_all_nov21.csv. Pick a city. Consider hotels and hostels. Consider all with at least 2 stars. You have 6 tasks (1p each). 
The goal of the exercise is to use information you have in your data to find a shortlist of five hotels and/or hostels that are good candidates for a good deal. You have to estimate a regression of prices (or log prices) on the other variables of your choice. You have to document your analysis and print the shortlist. 
</span>

First we load the file, then pick up a city randomly (happens to be Athens). Then filters to 'Hostel' and 'Hotel' types with minimum 2 stars. We then do some data cleaning, such as:
- dropping those few that miss parameters that we might be interested in,
- remove two duplicates.

``` {r Q22_basic}
#### Data Loading
file <- 'hotels_all_nov21.csv'
# exercise parameters
MIN_STARS <-2
hotels_all <- fread (file)
str (hotels_all)

#### Selecting a city randomly
citycount <- length(hotels_all [, unique(city)])
set.seed (28)
selectindex <- round(runif (1, min =1, max = citycount),0)
mycity <- hotels_all [, unique(city)][selectindex]
mycity

#### Filtering to Accomodation type and star
#checking if accomodation types, to check if there are various spellings of "hotel" and "hostel" in the dataset
hotels_all [city == mycity & stars >=2, unique (accommodation_type)]
#filtering to selected city and requested star and accomodation types
myhotels <- hotels_all [city == mycity & stars >=MIN_STARS & accommodation_type %in% c("Hotel", "Hostel"),]
# str (myhotels)

#### Data Cleaning 1: dropping those with missing parameters
myhotels [is.na (distance) | is.na(price) | is.na(stars) | is.na(rating) | is.na(badge_excellence) | is.na(rating_reviewcount),]
myhotels <- myhotels [!(is.na (distance) | is.na(price) | is.na(stars) | is.na(rating) | is.na(badge_excellence) | is.na(rating_reviewcount)),]

#### Data Cleaning 2: dropping duplicates
# looking for duplicates in some ways (ID, name, address)
myhotels [,.N] - myhotels [,length(unique (address))]
myhotels [, .N] - myhotels [,length(unique (name)) ] 
# showing the duplicates
myhotels [,.(name, .N), by = id][N>1,]
myhotels [,.(name, .N), by = address][N>1,]
myhotels [,.(address, .N), by = name][N>1,]
# checking the other 2 on the list with identical names & addresses
myhotels [name %in% c("Athens Lotus Hotel", "Metropolitan Hotel")]
myhotels1 <- myhotels[!duplicated(myhotels[,id]),]
```
We assumed that 'distance' is the distance from the city center


Next step is to filter extreme values.
- remove those that are considered to be outside of Athens (apparently all those that are further away than the hotel with the highest distance in Piraeuswhich means that we conside everything that is further away then Piraeus as suburb not Athens).
- we do not drop any hotel due to high price.

``` {r Q2_extremevalues}
max_in <- myhotels1 [city_actual == "Piraeus", max (distance)]
myhotels2 <- myhotels1[distance <= max_in, ]
myhotels2 [price >200,]  #checking who the very expensive ones are. After analyzing on web, we found tht these all are "normal" high priced hotels, we keep those.
myhotels3 <- myhotels2 [, .(distance, stars, badge_excellence, rating, rating_reviewcount, price)] #remove not needed data

```

### <span style="color:blue"> 2.2.1. Pick a set of variables. Describe all variables used in the analysis.
</span>

!!! I guess these are distance, stars, rating, badge_excellence, rating_reviewcount, price
!!!BLA-BLA-BLA

### <span style="color:blue"> 2.2.2. Investigate potential nonlinearity of each explanatory variable in simple regressions of the dependent variable. Decide on a parametric functional form for each.

Amiket szerintem érdemes megnézni:
lev-lev
lev-log
quadratic
cubic

``` {r Q2_nonlinears}
### Creating binary variables out of of badge_excellence
myhotels3 [, Gold := (badge_excellence == "Gold Award 2017") + 1]
myhotels3 [, Loved := (badge_excellence == "Loved by guests") + 1]
myhotels3 [, Top := (badge_excellence == "Top Hotel") + 1]

### Function to create R-square for 4 models
R2s <- function(x ,y){
  tmpy <- y [which (x > 0)]   #to eliminate log (0) 
  tmpx <- x [which (x > 0)]   #to eliminate log(0)
  logx <- log(tmpx)    #to eliminate log(0)
  sqx <- x ^2
  cubx <- x^3
  R2lin <- summary(lm(y~x))$r.squared
  R2log <- summary(lm(tmpy~logx))$r.squared   #to eliminate log(0)
  R2sqx <- summary(lm(y~x+sqx))$r.squared
  R2cub <- summary(lm(y~x+sqx+cubx))$r.squared
  return(c(R2lin, R2log,R2sqx, R2cub))
}

R2Table <- data.table (c("lev - lev lin", "lev - log", "Quadratic", "Qubic"), 
                          R2s (myhotels3 [, distance], myhotels3 [, price]), 
                          R2s (myhotels3 [, stars], myhotels3 [, price]),
                          R2s (myhotels3 [, Gold], myhotels3 [, price]),
                          R2s (myhotels3 [, Loved], myhotels3 [, price]),
                          R2s (myhotels3 [, Top], myhotels3 [, price]),
                          R2s (myhotels3 [, rating], myhotels3 [, price])
                          )
names(R2Table) <- c("RegType", "Distance", "Stars", "Gold", "Loved","Top", "Rating")
R2Table

```

3. Estimate a multiple regression with all explanatory variables in the functional form you specified previously.  

4. Pick two slope coefficients, interpret them, and compute and interpret their 95% CI. 

5. Describe your strategy to find the best deal. 

6. List the hotels with the smallest (most negative) residuals. List their prices and other characteristics as well. Comment on the results. 

Problem 2.3

- Pick another city.

- Estimate the model of your choice in the previous exercise (ie the exact same dependent variables, same functional form) for another city of your choice. Discuss your finding.

test
